{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Social_Distancing_Monitor.ipynb","provenance":[{"file_id":"https://github.com/rohanrao619/Social_Distancing_with_AI/blob/master/Social_Distancing_Monitor.ipynb","timestamp":1608447405141}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"fd21ed18749640589ade2d2773d1c798":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_174ab65cc269456e8e2c7d504f39c258","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_706ae28007a44f39836b0a5cef3e3c05","IPY_MODEL_6bcff01411ad4c4fbb69accae7868733"]}},"174ab65cc269456e8e2c7d504f39c258":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"706ae28007a44f39836b0a5cef3e3c05":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4c64cc5343784920a8a0d75f1a9a8572","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":481004605,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":481004605,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5114975354a44eaf95790ebf865094b6"}},"6bcff01411ad4c4fbb69accae7868733":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a0a5c61061f24cc9a5a1b2695c3d8022","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 459M/459M [00:47&lt;00:00, 10.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2c968cd332e546dfbb1044ab52f7ea1d"}},"4c64cc5343784920a8a0d75f1a9a8572":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5114975354a44eaf95790ebf865094b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a0a5c61061f24cc9a5a1b2695c3d8022":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2c968cd332e546dfbb1044ab52f7ea1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"Ygq4ZQabrDEE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608477391476,"user_tz":-330,"elapsed":20190,"user":{"displayName":"Raghav Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFLh4jMc80GsjgLSovXm1ndQR2gG9tDA2OxlD_mA=s64","userId":"07438410666498501470"}},"outputId":"a6fae5b5-a63e-4115-b718-dde3d7c1fd24"},"source":["# Mount Google Drive (If using Colab)\n","\n","from google.colab import drive\n","drive.mount(\"drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tvGhWHSHfU8B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608477397503,"user_tz":-330,"elapsed":26208,"user":{"displayName":"Raghav Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFLh4jMc80GsjgLSovXm1ndQR2gG9tDA2OxlD_mA=s64","userId":"07438410666498501470"}},"outputId":"47f64c26-891e-4500-dded-0a447552dab5"},"source":["# Install Required Libraries from PyPI\n","\n","!pip install face-detection\n","!pip install tqdm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting face-detection\n","  Downloading https://files.pythonhosted.org/packages/3c/ea/5e9af51646c5d7702c5223eb970b3b870c1de194260f9d6f61f9818bfe91/face_detection-0.2.1.tar.gz\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.6/dist-packages (from face-detection) (1.7.0+cu101)\n","Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from face-detection) (0.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face-detection) (1.19.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->face-detection) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->face-detection) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->face-detection) (0.8)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->face-detection) (7.0.0)\n","Building wheels for collected packages: face-detection\n","  Building wheel for face-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-detection: filename=face_detection-0.2.1-cp36-none-any.whl size=21404 sha256=9ef2a377b85b002f0571a2f60fbfa79663950a783c6423c390ad5182d251ef17\n","  Stored in directory: /root/.cache/pip/wheels/7d/45/54/d117d6cc260f31a22d15cda860ff46325af314cac687848ab7\n","Successfully built face-detection\n","Installing collected packages: face-detection\n","Successfully installed face-detection-0.2.1\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KgpQJHNSfsK6"},"source":["# Import Required Libraries\n","\n","import os\n","import numpy as np\n","import cv2\n","import face_detection \n","from sklearn.cluster import DBSCAN\n","from keras.models import load_model\n","from keras.applications.resnet50 import preprocess_input\n","import tqdm\n","from google.colab.patches import cv2_imshow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ygki7lR_sFvf"},"source":["# Path to the Working Environment\n","\n","# If using Google Colab (If on a Local Environment, no path required => set BASE_PATH  = \"\")\n","BASE_PATH = \"drive/My Drive/Colab Notebooks/\"\n","\n","# Path to Input Video File in the BASE_PATH\n","FILE_PATH = \"test_video.mp4\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xrRSzoT9EBa","colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["fd21ed18749640589ade2d2773d1c798","174ab65cc269456e8e2c7d504f39c258","706ae28007a44f39836b0a5cef3e3c05","6bcff01411ad4c4fbb69accae7868733","4c64cc5343784920a8a0d75f1a9a8572","5114975354a44eaf95790ebf865094b6","a0a5c61061f24cc9a5a1b2695c3d8022","2c968cd332e546dfbb1044ab52f7ea1d"]},"executionInfo":{"status":"ok","timestamp":1608477454360,"user_tz":-330,"elapsed":83053,"user":{"displayName":"Raghav Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFLh4jMc80GsjgLSovXm1ndQR2gG9tDA2OxlD_mA=s64","userId":"07438410666498501470"}},"outputId":"5b4a78b2-885f-4e3d-b22b-b926da695708"},"source":["# Initialize a Face Detector \n","# Confidence Threshold can be Adjusted, Greater values would Detect only Clear Faces\n","\n","detector = face_detection.build_detector(\"DSFDDetector\", confidence_threshold=.5, nms_iou_threshold=.3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"http://folk.ntnu.no/haakohu/WIDERFace_DSFD_RES152.pth\" to /root/.cache/torch/hub/checkpoints/WIDERFace_DSFD_RES152.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd21ed18749640589ade2d2773d1c798","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=481004605.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kv-5woacC0C5","colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"status":"error","timestamp":1608478568175,"user_tz":-330,"elapsed":1324,"user":{"displayName":"Raghav Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFLh4jMc80GsjgLSovXm1ndQR2gG9tDA2OxlD_mA=s64","userId":"07438410666498501470"}},"outputId":"87073d99-6b46-4a59-b25e-3f1d9c5fd5f7"},"source":["# Load Pretrained Face Mask Classfier (Keras Model)\n","\n","mask_classifier = load_model(\"drive/My Drive/Colab Notebooks/Models/ResNet50_Classifier.h5\")"],"execution_count":7,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-f4ba6a091bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Pretrained Face Mask Classfier (Keras Model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmask_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My Drive/Colab Notebooks/Models/ResNet50_Classifier.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: drive/My Drive/Colab Notebooks/Models/ResNet50_Classifier.h5/{saved_model.pbtxt|saved_model.pb}"]}]},{"cell_type":"code","metadata":{"id":"ehrozmEdySS1"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8O6mF_orCxN4"},"source":["# Set the Safe Distance in Pixel Units (Minimum Distance Expected to be Maintained between People)\n","# This Parameter would Affect the Results, Adjust according to the Footage captured by CCTV Camera \n","\n","threshold_distance = 150  # Try with different Values before Finalizing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejHxuy-9iZwL"},"source":["##################################### Analyze the Video ################################################\n","\n","# Load YOLOv3\n","net = cv2.dnn.readNet(BASE_PATH+\"Models/\"+\"yolov3.weights\", BASE_PATH+\"Models/\"+\"yolov3.cfg\")\n","\n","# Load COCO Classes\n","classes = []\n","with open(BASE_PATH+\"Models/\"+\"coco.names\", \"r\") as f:\n","    classes = [line.strip() for line in f.readlines()]\n","    \n","layer_names = net.getLayerNames()\n","output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n","\n","# Fetch Video Properties\n","cap = cv2.VideoCapture(BASE_PATH + FILE_PATH )\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n","height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n","\n","# Create Directory for Storing Results (Make sure it doesn't already exists !)\n","os.mkdir(BASE_PATH+\"Results\")\n","os.mkdir(BASE_PATH+\"Results/Extracted_Faces\")\n","os.mkdir(BASE_PATH+\"Results/Extracted_Persons\")\n","os.mkdir(BASE_PATH+\"Results/Frames\")\n","\n","# Initialize Output Video Stream\n","out_stream = cv2.VideoWriter(\n","    BASE_PATH+'Results/Output.mp4',\n","    cv2.VideoWriter_fourcc('X','V','I','D'),\n","    fps,\n","    (int(width),int(height)))\n","\n","print(\"Processing Frames :\")\n","for frame in tqdm.notebook.tqdm(range(int(n_frames))):\n","    \n","    # Capture Frame-by-Frame\n","    ret, img = cap.read()\n","\n","    # Check EOF\n","    if ret == False:\n","        break;\n","\n","    # Get Frame Dimentions\n","    height, width, channels = img.shape\n","\n","    # Detect Objects in the Frame with YOLOv3\n","    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n","    net.setInput(blob)\n","    outs = net.forward(output_layers)\n","\n","    class_ids = []\n","    confidences = []\n","    boxes = []\n","    \n","    # Store Detected Objects with Labels, Bounding_Boxes and their Confidences\n","    for out in outs:\n","        for detection in out:\n","            scores = detection[5:]\n","            class_id = np.argmax(scores)\n","            confidence = scores[class_id]\n","            if confidence > 0.5:\n","                \n","                # Get Center, Height and Width of the Box\n","                center_x = int(detection[0] * width)\n","                center_y = int(detection[1] * height)\n","                w = int(detection[2] * width)\n","                h = int(detection[3] * height)\n","\n","                # Topleft Co-ordinates\n","                x = int(center_x - w / 2)\n","                y = int(center_y - h / 2)\n","\n","                boxes.append([x, y, w, h])\n","                confidences.append(float(confidence))\n","                class_ids.append(class_id)\n","\n","    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","\n","    # Initialize empty lists for storing Bounding Boxes of People and their Faces\n","    persons = []\n","    masked_faces = []\n","    unmasked_faces = []\n","\n","    # Work on Detected Persons in the Frame\n","    for i in range(len(boxes)):\n","        if i in indexes:\n","\n","            box = np.array(boxes[i])\n","            box = np.where(box<0,0,box)\n","            (x, y, w, h) = box\n","\n","            label = str(classes[class_ids[i]])\n","\n","            if label=='person':\n","\n","                persons.append([x,y,w,h])\n","                \n","                # Save Image of Cropped Person (If not required, comment the command below)\n","                cv2.imwrite(BASE_PATH + \"Results/Extracted_Persons/\"+str(frame)\n","                            +\"_\"+str(len(persons))+\".jpg\",\n","                            img[y:y+h,x:x+w])\n","\n","                # Detect Face in the Person\n","                person_rgb = img[y:y+h,x:x+w,::-1]   # Crop & BGR to RGB\n","                detections = detector.detect(person_rgb)\n","\n","                # If a Face is Detected\n","                if detections.shape[0] > 0:\n","\n","                  detection = np.array(detections[0])\n","                  detection = np.where(detection<0,0,detection)\n","\n","                  # Calculating Co-ordinates of the Detected Face\n","                  x1 = x + int(detection[0])\n","                  x2 = x + int(detection[2])\n","                  y1 = y + int(detection[1])\n","                  y2 = y + int(detection[3])\n","\n","                  try :\n","\n","                    # Crop & BGR to RGB\n","                    face_rgb = img[y1:y2,x1:x2,::-1]   \n","\n","                    # Preprocess the Image\n","                    face_arr = cv2.resize(face_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)\n","                    face_arr = np.expand_dims(face_arr, axis=0)\n","                    face_arr = preprocess_input(face_arr)\n","\n","                    # Predict if the Face is Masked or Not\n","                    score = mask_classifier.predict(face_arr)\n","\n","                    # Determine and store Results\n","                    if score[0][0]<0.5:\n","                      masked_faces.append([x1,y1,x2,y2])\n","                    else:\n","                      unmasked_faces.append([x1,y1,x2,y2])\n","\n","                    # Save Image of Cropped Face (If not required, comment the command below)\n","                    cv2.imwrite(BASE_PATH + \"Results/Extracted_Faces/\"+str(frame)\n","                                +\"_\"+str(len(persons))+\".jpg\",\n","                                img[y1:y2,x1:x2])\n","\n","                  except:\n","                    continue\n","    \n","    # Calculate Coordinates of People Detected and find Clusters using DBSCAN\n","    person_coordinates = []\n","\n","    for p in range(len(persons)):\n","      person_coordinates.append((persons[p][0]+int(persons[p][2]/2),persons[p][1]+int(persons[p][3]/2)))\n","\n","    clustering = DBSCAN(eps=threshold_distance,min_samples=2).fit(person_coordinates)\n","    isSafe = clustering.labels_\n","\n","    # Count \n","    person_count = len(persons)\n","    masked_face_count = len(masked_faces)\n","    unmasked_face_count = len(unmasked_faces)\n","    safe_count = np.sum((isSafe==-1)*1)\n","    unsafe_count = person_count - safe_count\n","\n","    # Show Clusters using Red Lines\n","    arg_sorted = np.argsort(isSafe)\n","\n","    for i in range(1,person_count):\n","\n","      if isSafe[arg_sorted[i]]!=-1 and isSafe[arg_sorted[i]]==isSafe[arg_sorted[i-1]]:\n","        cv2.line(img,person_coordinates[arg_sorted[i]],person_coordinates[arg_sorted[i-1]],(0,0,255),2)\n","\n","    # Put Bounding Boxes on People in the Frame\n","    for p in range(person_count):\n","\n","      a,b,c,d = persons[p]\n","\n","      # Green if Safe, Red if UnSafe\n","      if isSafe[p]==-1:\n","        cv2.rectangle(img, (a, b), (a + c, b + d), (0,255,0), 2)\n","      else:\n","        cv2.rectangle(img, (a, b), (a + c, b + d), (0,0,255), 2)\n","\n","    # Put Bounding Boxes on Faces in the Frame\n","    # Green if Safe, Red if UnSafe\n","    for f in range(masked_face_count):\n","\n","      a,b,c,d = masked_faces[f]\n","      cv2.rectangle(img, (a, b), (c,d), (0,255,0), 2)\n","\n","    for f in range(unmasked_face_count):\n","\n","      a,b,c,d = unmasked_faces[f]\n","      cv2.rectangle(img, (a, b), (c,d), (0,0,255), 2)\n","\n","    # Show Monitoring Status in a Black Box at the Top\n","    cv2.rectangle(img,(0,0),(width,50),(0,0,0),-1)\n","    cv2.rectangle(img,(1,1),(width-1,50),(255,255,255),2)\n","\n","    xpos = 15\n","\n","    string = \"Total People = \"+str(person_count)\n","    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n","    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n","\n","    string = \" ( \"+str(safe_count) + \" Safe \"\n","    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n","    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n","\n","    string = str(unsafe_count)+ \" Unsafe ) \"\n","    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n","    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n","    \n","    string = \"( \" +str(masked_face_count)+\" Masked \"+str(unmasked_face_count)+\" Unmasked \"+\\\n","             str(person_count-masked_face_count-unmasked_face_count)+\" Unknown )\"\n","    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,255),2)\n","\n","    # Write Frame to the Output File\n","    out_stream.write(img)\n","\n","    # Save the Frame in frame_no.png format (If not required, comment the command below)\n","    cv2.imwrite(BASE_PATH+\"Results/Frames/\"+str(frame)+\".jpg\",img)\n","\n","    # Use if you want to see Results Frame by Frame\n","    # cv2_imshow('results',img)\n","\n","    # Exit on Pressing Q Key\n","    if cv2.waitKey(25) & 0xFF == ord('q'):\n","        break\n","\n","# Release Streams\n","out_stream.release()\n","cap.release()\n","cv2.destroyAllWindows()\n","\n","# Good to Go!\n","print(\"Done !\")"],"execution_count":null,"outputs":[]}]}